{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Information Gain is a metric used in Decision Trees to decide which feature should be used to split the data at each node.\n",
        "It measures how much uncertainty (entropy) is reduced after splitting the dataset based on a feature.\n",
        "\n",
        "The feature with the highest Information Gain is selected for splitting because it gives the most useful information.\n",
        "____"
      ],
      "metadata": {
        "id": "KHnzHqlbZF_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "answer\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Gini Impurity is a metric used in Decision Trees to measure how impure or mixed the classes are within a dataset after a split. It represents the probability that a randomly selected data point would be incorrectly classified if it were labeled according to the class distribution of that node. Gini Impurity is computationally simple and faster to calculate, which makes it suitable for large datasets and practical machine learning applications. Due to its efficiency, it is commonly used in the CART algorithm and in libraries such as scikit-learn. A lower Gini value indicates a purer node.\n",
        "\n",
        "Entropy\n",
        "\n",
        "Entropy is a measure of uncertainty or randomness in a dataset and is based on information theory. It calculates the amount of information required to describe the class distribution using logarithmic functions. Entropy provides a more theoretical and detailed understanding of data impurity but is computationally slower compared to Gini Impurity due to its complex calculations. It is mainly used in decision tree algorithms such as ID3 and C4.5. A lower entropy value signifies higher purity, while a higher value indicates more disorder in the dataset.\n",
        "___"
      ],
      "metadata": {
        "id": "oLbTwUQ-ZGH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        " Answer\n",
        "\n",
        "Pre-Pruning is a technique used in Decision Trees to prevent the model from becoming too complex and overfitting the training data. In this method, the growth of the decision tree is stopped early by setting certain conditions such as maximum depth of the tree, minimum number of samples required to split a node, or minimum samples required at a leaf node. By applying these limits during the training process, Pre-Pruning helps improve the generalization ability of the model on unseen data. It also reduces computation time and makes the model easier to interpret, although excessive pre-pruning may sometimes lead to underfitting.\n",
        "___"
      ],
      "metadata": {
        "id": "JDSDRkRyabA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity and print feature importances\n",
        "# Answer\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create Decision Tree model using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxuD0TJta5D8",
        "outputId": "f4adbc44-0226-4eef-b190-a5d255e1fc10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "[0.         0.01333333 0.06405596 0.92261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "_____\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer\n",
        "\n",
        " - Support Vector Machine (SVM) is a supervised machine learning algorithm.\n",
        "\n",
        "- It is used for classification and regression problems.\n",
        "\n",
        "- SVM works by finding the best separating boundary, called a hyperplane.\n",
        "\n",
        "- The hyperplane separates different classes with the maximum margin.\n",
        "\n",
        "- Data points that lie closest to the hyperplane are called support vectors.\n",
        "\n",
        "- SVM performs well for both linear and non-linear data.\n",
        "\n",
        "- It is effective in high-dimensional spaces."
      ],
      "metadata": {
        "id": "00r8SRsQbPLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "____\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "Answer :\n",
        "\n",
        "- The Kernel Trick is a technique used in Support Vector Machines (SVM).\n",
        "\n",
        "- It helps SVM handle non-linear data.\n",
        "\n",
        "- Kernel Trick transforms data into a higher-dimensional space.\n",
        "\n",
        "- This transformation makes non-linear data linearly separable.\n",
        "\n",
        "- The actual transformation is done implicitly, without calculating new features directly.\n",
        "\n",
        "- Common kernel functions include:\n",
        "\n",
        "  - Linear Kernel\n",
        "\n",
        "   - Polynomial Kernel\n",
        "\n",
        "   - Radial Basis Function (RBF)\n",
        "\n",
        "- Kernel Trick improves SVM’s performance on complex datasets.\n",
        "____"
      ],
      "metadata": {
        "id": "72MsCq6kbid5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Train two SVM classifiers with Linear and RBF kernels and compare their accuracies\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "linear_accuracy = accuracy_score(y_test, linear_pred)\n",
        "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2bJHFt-b7Uj",
        "outputId": "d16ecbba-63e4-4082-d6c9-aa0c1ec78daa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "______\n",
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer :\n",
        "\n",
        "- aïve Bayes is a supervised machine learning classifier.\n",
        "\n",
        "- It is based on Bayes’ Theorem.\n",
        "\n",
        "- It works on probability concepts to make predictions.\n",
        "\n",
        "- It assumes that all features are independent of each other.\n",
        "\n",
        "- This assumption is usually not true in real-life data.\n",
        "\n",
        "- Because of this strong assumption, it is called \"Naïve\".\n",
        "\n",
        "- Naïve Bayes is fast, simple, and efficient.\n",
        "\n",
        "- It performs well on large datasets and text classification problems.\n",
        "_______"
      ],
      "metadata": {
        "id": "go0YTjGAcL95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "✅ Answer:\n",
        "\n",
        "- Naïve Bayes has three main types depending on the type of data:\n",
        "\n",
        "1 Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "- Used for continuous numeric data.\n",
        "\n",
        "- Assumes that features follow a normal (Gaussian) distribution.\n",
        "\n",
        "- Example: Height, weight, or other measurements.\n",
        "\n",
        "2 Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "- Used for discrete/count data, often in text classification.\n",
        "\n",
        "- Works with word counts or frequencies.\n",
        "\n",
        "- Example: Spam email detection, document classification.\n",
        "\n",
        "3 Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "- Used for binary data (0 or 1 features).\n",
        "\n",
        "- Suitable for presence or absence of a feature.\n",
        "\n",
        "- Example: Whether a word appears in a document (Yes/No).\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "  - Gaussian → continuous features\n",
        "\n",
        "-  Multinomial → count-based features\n",
        "\n",
        "-  Bernoulli → binary features\n",
        "_____"
      ],
      "metadata": {
        "id": "PfzNMvmDctCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L9utmrTdOEa",
        "outputId": "676f882e-075a-4c8f-995b-a0e1a0c28e6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}